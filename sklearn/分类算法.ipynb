{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16aac011",
   "metadata": {},
   "source": [
    "### 1.2 估计器(sklearn机器学习算法的实现)\n",
    "在sklearn中，估计器(estimator)是一个重要的角色，是一类实现了算法的API\n",
    "\n",
    "#### 1、用于分类的估计器：\n",
    "sklearn.neighbors k-近邻算法\n",
    "sklearn.naive_bayes 贝叶斯\n",
    "sklearn.linear_model.LogisticRegression 逻辑回归\n",
    "sklearn.tree 决策树与随机森林\n",
    "#### 2、用于回归的估计器：\n",
    "sklearn.linear_model.LinearRegression 线性回归\n",
    "sklearn.linear_model.Ridge 岭回归\n",
    "#### 3、用于无监督学习的估计器\n",
    "sklearn.cluster.KMeans 聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b71f6",
   "metadata": {},
   "source": [
    "### 案例1：鸢尾花种类预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e24001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def knn_iris():\n",
    "    \"\"\"\n",
    "    用knn算法对鸢尾花进行分类\n",
    "    \"\"\"\n",
    "    # 1）获取数据\n",
    "    iris = load_iris()\n",
    "    \n",
    "    # 2）划分数据集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.2, random_state = 22)\n",
    "    \n",
    "    # 3）特征工程\n",
    "    transfer = StandardScaler()\n",
    "    x_train = transfer.fit_transform(x_train)\n",
    "    x_test = transfer.transform(x_test)\n",
    "    \n",
    "    # 4）KNN算法预估器\n",
    "    estimator = KNeighborsClassifier(n_neighbors=16)\n",
    "    estimator.fit(x_train, y_train)\n",
    "    \n",
    "    # 5）模型评估\n",
    "    #法1:直接比对真实值和预测值\n",
    "    y_predict = estimator.predict(x_test)\n",
    "    print(\"y_predict:\\n\", y_predict)\n",
    "    print(\"y_predict == y_test\\n\", y_predict == y_test)\n",
    "    \n",
    "    #法2:计算准确率\n",
    "    score = estimator.score(x_test, y_test)\n",
    "    print(\"score:\", score)\n",
    "    \n",
    "    \n",
    "#对鸢尾花案例加上K值调优\n",
    "def knn_iris_gscv():\n",
    "    \"\"\"\n",
    "    用knn算法对鸢尾花进行分类,添加网格搜索和交叉验证\n",
    "    \"\"\"\n",
    "    # 1）获取数据\n",
    "    iris = load_iris()\n",
    "    \n",
    "    # 2）划分数据集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.2, random_state = 22)\n",
    "    \n",
    "    # 3）特征工程\n",
    "    transfer = StandardScaler()\n",
    "    x_train = transfer.fit_transform(x_train)\n",
    "    x_test = transfer.transform(x_test)\n",
    "    \n",
    "    # 4）KNN算法预估器\n",
    "    estimator = KNeighborsClassifier()\n",
    "    \n",
    "    #添加网格搜索和交叉验证\n",
    "    param_dict = {\"n_neighbors\":[1,3,5,7,9,11,13]}\n",
    "    estimator = GridSearchCV(estimator,  param_grid = param_dict, cv=10)\n",
    "    \n",
    "    estimator.fit(x_train, y_train)\n",
    "    \n",
    "    # 5）模型评估\n",
    "    #法1:直接比对真实值和预测值\n",
    "    y_predict =  estimator.predict(x_test)\n",
    "    print(\"y_predict:\\n\", y_predict)\n",
    "    print(\"y_predict == y_test\\n\", y_predict == y_test)\n",
    "    \n",
    "    #法2:计算准确率 \n",
    "    score = estimator.score(x_test, y_test)\n",
    "    print(\"score:\", score)\n",
    "    \n",
    "    #最佳参数 best_param_\n",
    "    print(\"best_params_:\", estimator.best_params_)\n",
    "    #最佳结果 best_score_ \n",
    "    print(\"best_score_:\", estimator.best_score_)\n",
    "    #最佳估计值 best_estimator_\n",
    "    print(\"best_estimator_:\", estimator.best_estimator_)\n",
    "    #交叉验证结果 cv_results_\n",
    "    print(\"cv_results_:\", estimator.cv_results_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd00a049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict:\n",
      " [0 2 1 2 1 1 1 2 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2]\n",
      "y_predict == y_test\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True]\n",
      "score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "knn_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "918458dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict:\n",
      " [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 2 2 2 2]\n",
      "y_predict == y_test\n",
      " [ True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True]\n",
      "score: 0.9333333333333333\n",
      "best_params_: {'n_neighbors': 5}\n",
      "best_score_: 0.9666666666666666\n",
      "best_estimator_: KNeighborsClassifier()\n",
      "cv_results_: {'mean_fit_time': array([0.00070319, 0.00029962, 0.0007025 , 0.00050523, 0.00049438,\n",
      "       0.0005981 , 0.00060277]), 'std_fit_time': array([0.00046046, 0.00045768, 0.00064673, 0.00050543, 0.00049453,\n",
      "       0.00048875, 0.00049229]), 'mean_score_time': array([0.00109503, 0.0011003 , 0.00099375, 0.00118659, 0.00110552,\n",
      "       0.0009963 , 0.00089726]), 'std_score_time': array([2.99924005e-04, 3.14113089e-04, 1.36463972e-05, 3.93445971e-04,\n",
      "       2.96504393e-04, 3.84260933e-06, 5.36969256e-04]), 'param_n_neighbors': masked_array(data=[1, 3, 5, 7, 9, 11, 13],\n",
      "             mask=[False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'n_neighbors': 1}, {'n_neighbors': 3}, {'n_neighbors': 5}, {'n_neighbors': 7}, {'n_neighbors': 9}, {'n_neighbors': 11}, {'n_neighbors': 13}], 'split0_test_score': array([1., 1., 1., 1., 1., 1., 1.]), 'split1_test_score': array([0.91666667, 0.91666667, 1.        , 1.        , 0.91666667,\n",
      "       0.91666667, 0.91666667]), 'split2_test_score': array([1.        , 0.91666667, 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        ]), 'split3_test_score': array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667]), 'split4_test_score': array([0.91666667, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        ]), 'split5_test_score': array([1., 1., 1., 1., 1., 1., 1.]), 'split6_test_score': array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.91666667]), 'split7_test_score': array([0.83333333, 0.83333333, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.75      ]), 'split8_test_score': array([0.91666667, 0.91666667, 0.91666667, 0.91666667, 0.91666667,\n",
      "       0.91666667, 0.83333333]), 'split9_test_score': array([1., 1., 1., 1., 1., 1., 1.]), 'mean_test_score': array([0.94166667, 0.94166667, 0.96666667, 0.96666667, 0.95833333,\n",
      "       0.95833333, 0.93333333]), 'std_test_score': array([0.05335937, 0.05335937, 0.04082483, 0.04082483, 0.04166667,\n",
      "       0.04166667, 0.08164966]), 'rank_test_score': array([5, 5, 1, 1, 3, 3, 7])}\n"
     ]
    }
   ],
   "source": [
    "knn_iris_gscv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c47dcd",
   "metadata": {},
   "source": [
    "### Facebook签到位置预测K值调优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d4be735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd1ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1获取数据\n",
    "data = pd.read_csv(\"./dataset/FBlocation/train.csv\")\n",
    "#2基本的数据处理\n",
    "#2.1缩小数据范围 \"x > 2 & x < 2.5 & y > 1 & y < 1.5\"\n",
    "#data = data.query(\"x > 2 & x < 2.5 & y > 1 & y < 1.5\")\n",
    "data = data[:, :100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    " #2.2处理时间特征\n",
    "time_value = pd.to_datetime(data[\"time\"], unit=\"s\")\n",
    "time_value.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.DatetimeIndex(time_value)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"day\"] = date.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"weekday\"] = date.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"hour\"] = date.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904f523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3过滤签到次数少的地点\n",
    "place_count = data.groupby(\"place_id\").count()[\"row_id\"]\n",
    "place_count[place_count > 3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb648f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data[data[\"place_id\"].isin(place_count[place_count > 3].index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076c2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#筛选特征值和目标值\n",
    "x = data_final[[\"x\", \"y\", \"accuracy\", \"day\", \"weekday\", \"hour\"]]\n",
    "y = data_final[\"place_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 3）特征工程\n",
    "transfer = StandardScaler()\n",
    "x_train = transfer.fit_transform(x_train)\n",
    "x_test = transfer.transform(x_test)\n",
    "\n",
    "# 4）KNN算法预估器\n",
    "estimator = KNeighborsClassifier()\n",
    "\n",
    "#添加网格搜索和交叉验证\n",
    "param_dict = {\"n_neighbors\":[3,5,7,9]}\n",
    "estimator = GridSearchCV(estimator,  param_grid = param_dict, cv=5)\n",
    "    \n",
    "estimator.fit(x_train, y_train)\n",
    "    \n",
    "# 5）模型评估\n",
    "#法1:直接比对真实值和预测值\n",
    "y_predict =  estimator.predict(x_test)\n",
    "print(\"y_predict:\\n\", y_predict)\n",
    "print(\"y_predict == y_test\\n\", y_predict == y_test)\n",
    " \n",
    "#法2:计算准确率 \n",
    "score = estimator.score(x_test, y_test)\n",
    "print(\"score:\", score)\n",
    "\n",
    "#最佳参数 best_param_\n",
    "print(\"best_params_:\", estimator.best_params_)\n",
    "#最佳结果 best_score_ \n",
    "print(\"best_score_:\", estimator.best_score_)\n",
    "#最佳估计值 best_estimator_\n",
    "print(\"best_estimator_:\", estimator.best_estimator_)\n",
    "#交叉验证结果 cv_results_\n",
    "print(\"cv_results_:\", estimator.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb7d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a1e6181",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a1eecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f8200a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_news():\n",
    "    \"\"\"\n",
    "    朴素贝叶斯对新闻数据集进行预测\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 获取新闻的数据，20个类别\n",
    "    news = fetch_20newsgroups()\n",
    "\n",
    "    # 进行数据集分割\n",
    "    x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.3)\n",
    "   \n",
    "    # 对于文本数据，进行特征抽取\n",
    "    tf = TfidfVectorizer()\n",
    "    \n",
    "    x_train = tf.fit_transform(x_train)\n",
    "    # 这里打印出来的列表是：训练集当中的所有不同词的组成的一个列表\n",
    "    print(tf.get_feature_names())\n",
    "    #print(x_train.toarray())\n",
    "\n",
    "\n",
    "    # 不能调用fit_transform\n",
    "    x_test = tf.trainsform(x_test)\n",
    "\n",
    "    # estimator估计器流程\n",
    "    mlb = MultinomialNB(alpha=1.0)\n",
    "    mlb.fit(x_train,  y_train)\n",
    "\n",
    "    # 进行预测\n",
    "    y_predict = mlb.predict(x_test)\n",
    "\n",
    "    print(\"预测每篇文章的类别：\",y_predict[:100])\n",
    "    print(\"真实类别为：\", y_test[:100])\n",
    "\n",
    "    print(\"预测准确率为：\", mlb.score(x_test, y_test))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14d5a19a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12452/91205138.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnb_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12452/2993956788.py\u001b[0m in \u001b[0;36mnb_news\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# 获取新闻的数据，20个类别\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mnews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 进行数据集分割\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py\u001b[0m in \u001b[0;36mfetch_20newsgroups\u001b[1;34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y)\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m80\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload_if_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py\u001b[0m in \u001b[0;36m_download_20newsgroups\u001b[1;34m(target_dir, cache_path)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m#logger.info(\"Downloading dataset from %s (14 MB)\", ARCHIVE.url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;31m#archive_path = _fetch_remote(ARCHIVE, dirname=target_dir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;31m#logger.debug(\"Decompressing %s\", archive_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m#tarfile.open(archive_path, \"r:gz\").extractall(path=target_dir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\_base.py\u001b[0m in \u001b[0;36m_fetch_remote\u001b[1;34m(remote, dirname)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     file_path = (remote.filename if dirname is None\n\u001b[0;32m   1193\u001b[0m                  else join(dirname, remote.filename))\n\u001b[1;32m-> 1194\u001b[1;33m     \u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m     \u001b[0mchecksum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sha256\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mremote\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchecksum\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mchecksum\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "nb_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94783c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "096b0041",
   "metadata": {},
   "source": [
    "### 决策树\n",
    "class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)\n",
    "\n",
    "决策树分类器\n",
    "\n",
    "    criterion:默认是’gini’系数，也可以选择信息增益的熵’entropy’\n",
    "    max_depth:树的深度大小\n",
    "    random_state:随机数种子\n",
    "其中会有些超参数：max_depth:树的深度大小\n",
    "其它超参数我们会结合随机森林讲解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ea6169b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70981e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree_iris():\n",
    "    \"\"\"\n",
    "    决策树进行鸢尾花分类\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #获取数据\n",
    "    iris = load_iris()\n",
    "    #划分数据集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.2, random_state=22)\n",
    "    #决策树预估器 (信息增益)\n",
    "    estimator = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "    estimator.fit(x_train, y_train)\n",
    "    \n",
    "    #模型评估\n",
    "    #法1:直接比对真实值和预测值\n",
    "    y_predict = estimator.predict(x_test)\n",
    "    print(\"y_predict:\\n\", y_predict)\n",
    "    print(\"y_predict == y_test\\n\", y_predict == y_test)\n",
    "    \n",
    "    #法2:计算准确率\n",
    "    score = estimator.score(x_test, y_test)\n",
    "    print(\"score:\", score)\n",
    "    \n",
    "    \n",
    "    ### 决策树可视化  （http://webgraphviz.com/     将.dot文件内容复制到这个网站实现可视化）\n",
    "    export_graphviz(estimator, out_file=\"iris_tree.dot\", feature_names = iris.feature_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5be82b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict:\n",
      " [0 2 1 2 1 1 1 1 1 0 2 1 2 2 0 2 1 1 1 1 0 2 0 1 2 0 1 2 2 1]\n",
      "y_predict == y_test\n",
      " [ True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True False  True  True False]\n",
      "score: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "decisionTree_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068876b7",
   "metadata": {},
   "source": [
    "### 案例：泰坦尼克号乘客生存预测 (决策树)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2fb33294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e0fa2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取数据\n",
    "path = \"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt\"\n",
    "\n",
    "train = pd.read_csv(\"dataset/taitanic/train.csv\")\n",
    "test = pd.read_csv(\"dataset/taitanic/test.csv\")\n",
    "\n",
    "\n",
    "#数据处理  缺失值处理  特征值—>字典类型\n",
    "\n",
    "#准备好特征值、目标值\n",
    "#划分数据集\n",
    "#特征工程：字典特征抽取\n",
    "#决策树预估器流程\n",
    "#模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "99068013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info() #查看数据集整体情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c7e568b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe() #了解数据集的统计情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2250f692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>347082</td>\n",
       "      <td>B96 B98</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name   Sex  Ticket    Cabin Embarked\n",
       "count                       891   891     891      204      889\n",
       "unique                      891     2     681      147        3\n",
       "top     Braund, Mr. Owen Harris  male  347082  B96 B98        S\n",
       "freq                          1   577       7        4      644"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(include=['O']) #查看非数字字段情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "987bb13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据处理  缺失值处理  特征值—>字典类型\n",
    "\n",
    "##训练集\n",
    "#缺失值处理 Age Cabin Embarked\n",
    "#Age中的空值可用平均年龄来填充\n",
    "train[\"Age\"].fillna(train[\"Age\"].mean(), inplace = True)\n",
    "\n",
    "#Cabin有大量的缺失值，在训练集和测试集中缺失率都比较高，无法补齐\n",
    "#Embarked为登陆港口，可以根据港口属性补齐。可以看到港口为“S”类型的占比最高，\n",
    "#train[\"Embarked\"].value_counts()\n",
    "#可以考虑把缺失的港口用“S”港口填充\n",
    "train[\"Embarked\"].fillna(\"S\", inplace=True)\n",
    "\n",
    "\n",
    "##测试集 Age、Fare、Cabin是有缺失数据的\n",
    "#Age中的空值可用平均年龄来填充\n",
    "test[\"Age\"].fillna(test[\"Age\"].mean(), inplace = True)\n",
    "#Fare中的空值可用平均票价来填充\n",
    "test[\"Fare\"].fillna(test[\"Fare\"].mean(), inplace = True)\n",
    "#Cabin有大量的缺失值，在训练集和测试集中缺失率都比较高，无法补齐\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "453e39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征选择\n",
    "features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
    "train_features = train[features]\n",
    "train_labels = train['Survived']\n",
    "test_features = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d704bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qjx\\AppData\\Local\\Temp/ipykernel_12452/3669045001.py:9: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  train_features = dv.fit_transform(train_features.to_dict(orient=\"record\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'Embarked=C',\n",
       " 'Embarked=Q',\n",
       " 'Embarked=S',\n",
       " 'Fare',\n",
       " 'Parch',\n",
       " 'Pclass',\n",
       " 'Sex=female',\n",
       " 'Sex=male',\n",
       " 'SibSp']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征工程：字典特征抽取\n",
    "#特征值有一些是字符串，不方便后续的运算，需要把它们转化为数值类型，\n",
    "\n",
    "#Sex 有male和female两种类型，可以把它变成 Sex=male 和 Sex = female， 数值用0或1来表示\n",
    "#Embarked 有S 、C 、Q三种类型，可以把它变成Embarked= S 、 Embarked= C、Embarked= Q，数值用0或1来表示\n",
    "#可以使用sklearn 特征选择中的 DictVectorizer类，用它可以处理符号化的对象，将符号转化为数字0或1进行表示\n",
    "\n",
    "dv = DictVectorizer(sparse = False)\n",
    "train_features = dv.fit_transform(train_features.to_dict(orient=\"record\"))\n",
    "\n",
    "dv.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "569794cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#决策树预估器流程\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "491bc49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qjx\\AppData\\Local\\Temp/ipykernel_12452/2261150618.py:2: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  test_features = dv.transform(test_features.to_dict(orient=\"record\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型评估\n",
    "test_features = dv.transform(test_features.to_dict(orient=\"record\"))\n",
    "pred_labels = clf.predict(test_features)\n",
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fcb8bd9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982043"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在模型评估中，决策树提供了score函数可以直接得到准确率。\n",
    "#但由于我们的测试集中并没有真实的生存状况的结果，\n",
    "#只能使用训练集中的数 据进行模型评估\n",
    "\n",
    "acc_decision_tree = round(clf.score(train_features, train_labels), 6)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ab787f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7756304619225968"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用K折交叉验证统计决策树分类器的准确率\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.mean(cross_val_score(clf, train_features, train_labels, cv = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e821d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3069641",
   "metadata": {},
   "source": [
    "### 随机森林\n",
    "class sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)\n",
    "\n",
    "#### 随机森林分类器\n",
    "    n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200\n",
    "    criteria：string，可选（default =“gini”）分割特征的测量方法\n",
    "    max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30\n",
    "    max_features=\"auto”,每个决策树的最大特征数量\n",
    "        If \"auto\", then max_features=sqrt(n_features).\n",
    "        If \"sqrt\", then max_features=sqrt(n_features) (same as \"auto\").\n",
    "        If \"log2\", then max_features=log2(n_features).\n",
    "        If None, then max_features=n_features.\n",
    "    bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样\n",
    "    min_samples_split:节点划分最少样本数\n",
    "    min_samples_leaf:叶子节点的最小样本数\n",
    "    \n",
    "超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2b876",
   "metadata": {},
   "source": [
    "### 泰坦尼克号乘客生存预测 (随机森林) (训练集和测试集用的是决策树的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b04ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4dee9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果： [0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 1 0 1 0 0 1 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.826079900124844"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "estimator = RandomForestClassifier()\n",
    "#添加网格搜索和交叉验证\n",
    "\n",
    "param_dict = {\"n_estimators\":[120,200,300,500,800,1200],\n",
    "              \"max_depth\": [5, 8, 15, 25, 30]}\n",
    "estimator = GridSearchCV(estimator,  param_grid = param_dict, cv=3)\n",
    "    \n",
    "estimator.fit(train_features, train_labels)\n",
    "\n",
    "pred_labels = estimator.predict(test_features)\n",
    "print(\"预测结果：\", pred_labels)\n",
    "\n",
    "# 5）模型评估\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.mean(cross_val_score(estimator, train_features, train_labels, cv = 10))\n",
    "\n",
    "# # 5）模型评估\n",
    "# #法1:直接比对真实值和预测值\n",
    "# y_predict =  estimator.predict(x_test)\n",
    "# print(\"y_predict:\\n\", y_predict)\n",
    "# print(\"y_predict == y_test\\n\", y_predict == y_test)\n",
    " \n",
    "# #法2:计算准确率 \n",
    "# score = estimator.score(x_test, y_test)\n",
    "# print(\"score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9333a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
